<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Video Frame Synthesis using Deep Voxel Flow</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art.">
<meta name="keywords" content="video frame synthesis; video interpolation; slo-mo effect; deep voxel flow; optical flow estimation; unsupervised learning; deep learning;">
<link rel="author" href="https://liuziwei7.github.io/">

<!-- Fonts and stuff -->
<link href="./voxelflow/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./voxelflow/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./voxelflow/iconize.css">
<script async="" src="./voxelflow/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>Video Frame Synthesis using Deep Voxel Flow</h1>

	<div class="authors">
	  <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.isle.illinois.edu/~yeh17/">Raymond A. Yeh</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml">Xiaoou Tang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://bitstream9.me/">Yiming Liu</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.agarwala.org/">Aseem Agarwala</a><sup>4</sup>
	</div>

	<div class="affiliations">
	  1. <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  2. <a href="http://illinois.edu/">University of Illinois at Urbana-Champaign</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  3. <a href="https://www.pony.ai/">Pony.AI Inc.</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  4. <a href="https://research.google.com/">Google Inc.</a>
	</div>

	<div class="venue">International Conference on Computer Vision (<a href="http://iccv2017.thecvf.com/" target="_blank">ICCV</a>) 2017, <font color="#e86e14">Oral Presentation</font> </div>
      
      </div>
      
      <center><img src="./voxelflow/intro.jpg" border="0" width="80%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art.
	</p>
      </div>

<div class="section demo">
	<h2>Public Video</h2>
	<br>
	<center>
	  <iframe width="810" height="480" src="https://www.youtube.com/embed/qNXPI01WlBU" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>

<br>

<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/1702.02463" target="_blank" class="imageLink"><img src="./voxelflow/paper.jpg"></a><br>
		  <a href="https://arxiv.org/abs/1702.02463" target="_blank">Paper</a>
		</div>
	      </li>

	      <li class="grid">
	      <div class="griditem">
		<a href="../papers/voxelflow_poster.pdf" target="_blank" class="imageLink"><img src="./voxelflow/poster.jpg"></a><br>
		  <a href="../papers/voxelflow_poster.pdf" target="_blank">Poster</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>

<br>

<div class="section presentation">
	<h2>Presentation</h2>
	<center>
	  <ul>
            <li class="grid">
	      <div class="griditem">
		<a href="https://www.youtube.com/watch?v=4Y30cawxK1w" target="_blank" class="imageLink"><img src="./voxelflow/video.png"></a><br>
		  <a href="https://www.youtube.com/watch?v=4Y30cawxK1w" target="_blank">Video Recording</a>
		</div>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="../papers/voxelflow_slides.pdf" target="_blank" class="imageLink"><img src="./voxelflow/slides.jpg"></a><br>
		  <a href="../papers/voxelflow_slides.pdf" target="_blank">Slides</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>

<br>

<div class="section code">
	<h2>Code and Models</h2>
	<center>
	<ul>
           
    <li class="grid">
	<div class="griditem">
	<a href="https://github.com/liuziwei7/voxel-flow" target="_blank" class="imageLink"><img src="./voxelflow/code.png"></a><br>
	<a href="https://github.com/liuziwei7/voxel-flow" target="_blank">Code and Models (TensorFlow)</a>
	</div>
	</li>

	<li class="grid">
	<div class="griditem">
	<a href="https://github.com/lxx1991/pytorch-voxel-flow" target="_blank" class="imageLink"><img src="./voxelflow/code.png"></a><br>
	<a href="https://github.com/lxx1991/pytorch-voxel-flow" target="_blank">Code and Models (PyTorch)</a>
	</div>
	</li>

	</ul>
	</center>    
	</div>

<br>

<div class="section visual">
	<h2>Visual Results</h2>
	<p><a href="./voxelflow/demo.html">Real-world 1080p Videos</a></p>
	<p><a href="./voxelflow/demo_ucf101.html">UCF-101 Sports Videos</a></p>
	</div>

<br>

<div class="section results">
	<h2>Datasets</h2>
    <center>
	  <ul>

	  <li class="grid">
	      <div class="griditem">
		<a href="https://drive.google.com/open?id=1rwnTfzCEIMFv6xiBGCpSnCUvMufJXdkU" target="_blank" class="imageLink"><img src="./voxelflow/icon_zip.png"></a><br>
		  <a href="https://drive.google.com/open?id=1rwnTfzCEIMFv6xiBGCpSnCUvMufJXdkU" target="_blank">Train/Test Split (UCF-101)</a>
		</div>
	      </li> 

	  <li class="grid">
	      <div class="griditem">
		<a href="https://drive.google.com/open?id=1Uc7ZPsiPf-ViuZusdmz5D4P8E5VewhH6" target="_blank" class="imageLink"><img src="./voxelflow/icon_zip.png"></a><br>
		  <a href="https://drive.google.com/open?id=1Uc7ZPsiPf-ViuZusdmz5D4P8E5VewhH6" target="_blank">Motion Masks (UCF-101)</a>
		</div>
	      </li> 

	  <li class="grid">
	      <div class="griditem">
		<a href="https://drive.google.com/open?id=0B7EVK8r0v71pdHBNdXB6TE1wSTQ" target="_blank" class="imageLink"><img src="./voxelflow/icon_zip.png"></a><br>
		  <a href="https://drive.google.com/open?id=0B7EVK8r0v71pdHBNdXB6TE1wSTQ" target="_blank">Video Interpolation Results (UCF-101)</a>
		</div>
	      </li> 

	  <li class="grid">
	      <div class="griditem">
		<a href="https://drive.google.com/open?id=1EDIn8gxHpApmVPzDts2zc45ixbnqAegp" target="_blank" class="imageLink"><img src="./voxelflow/icon_zip.png"></a><br>
		  <a href="https://drive.google.com/open?id=1EDIn8gxHpApmVPzDts2zc45ixbnqAegp" target="_blank">Video Extrapolation Results (UCF-101)</a>
		</div>
	      </li> 

	 	</ul>
	    </center>
	    </div>

<br>

<div class="section video">
	<h2>Paper Video</h2>
	<br>
	<center>
	  <iframe width="810" height="480" src="https://www.youtube.com/embed/7otKtmT-vjI" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>
	    
<br>

<div class="section product">
	<h2>Product Transfer</h2><center>
		<br>
      	<a href="https://www.theverge.com/2017/10/4/16405200/google-clips-camera-ai-photos-video-hands-on-wi-fi-direct" target="_blank" class="imageLink"><img src="./voxelflow/product.jpg" border="2" width="70%"></a><br>
      	<a href="https://www.theverge.com/2017/10/4/16405200/google-clips-camera-ai-photos-video-hands-on-wi-fi-direct" target="_blank">Google Clips</a>
      </div></center>

<br>

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@inproceedings{liu2017voxelflow,
 author = {Ziwei Liu, Raymond Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala},
 title = {Video Frame Synthesis using Deep Voxel Flow},
 booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
 month = {October},
 year = {2017} 
}</pre>
	  </div>
      </div>

</body></html>