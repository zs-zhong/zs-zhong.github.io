<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Unconstrained Fashion Landmark Detection via Hierarchical Recurrent Transformer Networks</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Fashion landmarks are functional key points defined on clothes, such as corners of neckline, hemline, and cuff. They have been recently introduced as an effective visual representation for fashion image understanding. However, detecting fashion landmarks are challenging due to background clutters, human poses, and scales. To remove the above variations, previous works usually assumed bounding boxes of clothes are provided in training and test as additional annotations, which are expensive to obtain and inapplicable in practice. This work addresses unconstrained fashion landmark detection, where clothing bounding boxes are not provided in both training and test. To this end, we present a novel Deep LAndmark Network (DLAN), where bounding boxes and landmarks are jointly estimated and trained iteratively in an end-to-end manner. DLAN contains two dedicated modules, including a Selective Dilated Convolution for handling scale discrepancies, and a Hierarchical Recurrent Spatial Transformer for handling background clutters. To evaluate DLAN, we present a large-scale fashion landmark dataset, namely Unconstrained Landmark Database (ULD), consisting of 30K images. Statistics show that ULD is more challenging than existing datasets in terms of image scales, background clutters, and human poses. Extensive experiments demonstrate the effectiveness of DLAN over the state-of-the-art methods. DLAN also exhibits excellent generalization across different clothing categories and modalities, making it extremely suitable for real-world fashion analysis.">
<meta name="keywords" content="fashion landmarks; unconstrained; spatial transformer; landmark detection; deep learning;">
<link rel="author" href="personal.ie.cuhk.edu.hk/~lz013/">

<!-- Fonts and stuff -->
<link href="./unconstrainedlandmarks/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./unconstrainedlandmarks/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./unconstrainedlandmarks/iconize.css">
<script async="" src="./unconstrainedlandmarks/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>Unconstrained Fashion Landmark Detection via Hierarchical Recurrent Transformer Networks</h1>

	<div class="authors">
	  <a href="">Sijie Yan</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://personal.ie.cuhk.edu.hk/~pluo/">Ping Luo</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://puzzledqs.github.io/">Shi Qiu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml">Xiaoou Tang</a>
	</div>

	<div class="affiliations">
	  <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory, </a>
	  <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>
	</div>

	<div class="venue">ACM Multimedia (<a href="http://www.acmmm.org/2017/" target="_blank">ACM MM</a>) 2017, Full Research Paper</div>
      </div>

      
      <center><img src="./unconstrainedlandmarks/intro.jpg" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
Fashion landmarks are functional key points defined on clothes, such as corners of neckline, hemline, and cuff. They have been recently introduced as an effective visual representation for fashion image understanding. However, detecting fashion landmarks are challenging due to background clutters, human poses, and scales. To remove the above variations, previous works usually assumed bounding boxes of clothes are provided in training and test as additional annotations, which are expensive to obtain and inapplicable in practice. This work addresses unconstrained fashion landmark detection, where clothing bounding boxes are not provided in both training and test. To this end, we present a novel Deep LAndmark Network (DLAN), where bounding boxes and landmarks are jointly estimated and trained iteratively in an end-to-end manner. DLAN contains two dedicated modules, including a Selective Dilated Convolution for handling scale discrepancies, and a Hierarchical Recurrent Spatial Transformer for handling background clutters. To evaluate DLAN, we present a large-scale fashion landmark dataset, namely Unconstrained Landmark Database (ULD), consisting of 30K images. Statistics show that ULD is more challenging than existing datasets in terms of image scales, background clutters, and human poses. Extensive experiments demonstrate the effectiveness of DLAN over the state-of-the-art methods. DLAN also exhibits excellent generalization across different clothing categories and modalities, making it extremely suitable for real-world fashion analysis.
	</p>
      </div>

<div class="section spotlight">
	<h2>Spotlight</h2><center>
	<br>
      	<iframe width="720" height="400" src="https://www.youtube.com/embed/LcRUSZjnDXA" frameborder="0" allowfullscreen></iframe>
      </div></center>

<br>

<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/FashionLandmarksMM17.pdf" target="_blank" class="imageLink"><img src="./unconstrainedlandmarks/paper.jpg"></a><br>
		  <a href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/FashionLandmarksMM17.pdf" target="_blank">Paper</a>
		</div>
		  </li>
         
          <li class="grid">
		  <div class="griditem">
		<a href="../papers/unconstrainedlandmarks_poster.pdf" target="_blank" class="imageLink"><img src="./unconstrainedlandmarks/poster.jpg"></a><br>
		  <a href="../papers/unconstrainedlandmarks_poster.pdf" target="_blank">Poster</a>
		</div>

	      </li>
	  
	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="section code">
	<h2>Code and Models</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/yysijie/DLAN/" target="_blank" class="imageLink"><img src="./unconstrainedlandmarks/code.png"></a><br>
		  <a href="https://github.com/yysijie/DLAN/" target="_blank">Code and Models</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="section demo">
	<h2>Demo Video</h2>
	<br>
	<center>
	  <iframe width="810" height="480" src="https://www.youtube.com/embed/uFpurN6dDiY" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>
	    
<br>

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@inproceedings{yan2017unconstrainedlandmarks,
 author = {Sijie Yan, Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang},
 title = {Unconstrained Fashion Landmark Detection via Hierarchical Recurrent Transformer Networks},
 booktitle = {ACM Multimedia (ACM MM)},
 month = {October},
 year = {2017} 
}</pre>
	  </div>
      </div>

</body></html>